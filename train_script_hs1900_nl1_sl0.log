Namespace(init_path='UniRep/1900_weights', n_layers=1, num_iters=1000, rnn_size=1900, save_dir='1000_iters_hs1900_nl4_pt_nosite', site_loss=True)
Uniprot data had already been processed. Loading from csv
WARNING:tensorflow:From /clusterfs/mhg/sprasad/CTA/geometries/UniRep_finetune_forest/UniRep/unirep.py:113: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.
Instructions for updating:
dim is deprecated, use axis instead
2020-05-07 14:26:44.388025: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-05-07 14:26:44.388100: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel driver does not appear to be running on this host (n0071.mhg0): /proc/driver/nvidia/version does not exist
Size of training data: 49839, size of test data: 12460
WARNING:tensorflow:From /global/software/sl-7.x86_64/modules/apps/ml/tensorflow/1.12.0-py36/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From /clusterfs/mhg/sprasad/CTA/geometries/UniRep_finetune_forest/UniRep/data_utils.py:190: group_by_window (from tensorflow.contrib.data.python.ops.grouping) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.group_by_window(...)`.
START OF TRAINING FOR 1000 ITERATIONS
Iteration 0, loss 2.6234827041625977, site loss 0.9441781044006348
Iteration 1, loss 2.6956725120544434, site loss 0.3234706521034241
Iteration 2, loss 2.7495293617248535, site loss 0.13136926293373108
Iteration 3, loss 2.737977981567383, site loss 0.10215909779071808
Iteration 4, loss 2.7028565406799316, site loss 0.09128598868846893
Iteration 5, loss 2.6815009117126465, site loss 0.0797245129942894
Iteration 6, loss 2.6816484928131104, site loss 0.07365264743566513
Iteration 7, loss 2.641002655029297, site loss 0.07248705625534058
Iteration 8, loss 2.6434550285339355, site loss 0.07293746620416641
Iteration 9, loss 2.6033365726470947, site loss 0.06840811669826508
Iteration 10, loss 2.6162848472595215, site loss 0.06793513149023056
Iteration 11, loss 2.6028428077697754, site loss 0.06849023699760437
Iteration 12, loss 2.571861505508423, site loss 0.064204640686512
Iteration 13, loss 2.5682878494262695, site loss 0.06583704054355621
Iteration 14, loss 2.552097797393799, site loss 0.06781286001205444
Iteration 15, loss 2.49238920211792, site loss 0.06048982962965965
Iteration 16, loss 2.5519423484802246, site loss 0.0610562264919281
Iteration 17, loss 2.4536938667297363, site loss 0.06104360893368721
Iteration 18, loss 2.4553794860839844, site loss 0.06372012197971344
Iteration 19, loss 2.4906563758850098, site loss 0.0644923746585846
Iteration 20, loss 2.472616195678711, site loss 0.05803631991147995
Iteration 21, loss 2.5136632919311523, site loss 0.05812995135784149
Iteration 22, loss 2.442586660385132, site loss 0.058690302073955536
Iteration 23, loss 2.4375360012054443, site loss 0.05245817452669144
Iteration 24, loss 2.456967830657959, site loss 0.0561203770339489
Iteration 25, loss 2.458639144897461, site loss 0.060030192136764526
Iteration 26, loss 2.44671368598938, site loss 0.05976442992687225
Iteration 27, loss 2.4256844520568848, site loss 0.056909285485744476
Iteration 28, loss 2.408620834350586, site loss 0.056215800344944
Iteration 29, loss 2.404109239578247, site loss 0.055636223405599594
Iteration 30, loss 2.4781830310821533, site loss 0.05577049404382706
Iteration 31, loss 2.3709723949432373, site loss 0.052567124366760254
Iteration 32, loss 2.3800954818725586, site loss 0.050674133002758026
Iteration 33, loss 2.4028773307800293, site loss 0.05916871502995491
Iteration 34, loss 2.369373321533203, site loss 0.05386262759566307
Iteration 35, loss 2.343520164489746, site loss 0.055959589779376984
Iteration 36, loss 2.3436148166656494, site loss 0.05245675519108772
Iteration 37, loss 2.355785846710205, site loss 0.057337962090969086
Iteration 38, loss 2.307624101638794, site loss 0.05410977452993393
Iteration 39, loss 2.3221821784973145, site loss 0.05233875662088394
Iteration 40, loss 2.365234851837158, site loss 0.05445051193237305
Iteration 41, loss 2.359689235687256, site loss 0.054369036108255386
Iteration 42, loss 2.2791433334350586, site loss 0.050376810133457184
Iteration 43, loss 2.2550361156463623, site loss 0.04895307496190071
Iteration 44, loss 2.342482328414917, site loss 0.05371161550283432
Iteration 45, loss 2.3191707134246826, site loss 0.052550412714481354
Iteration 46, loss 2.367231845855713, site loss 0.051671966910362244
Iteration 47, loss 2.325249671936035, site loss 0.054074548184871674
Iteration 48, loss 2.345008134841919, site loss 0.056325651705265045
Iteration 49, loss 2.3131134510040283, site loss 0.05035260319709778
Iteration 50, loss 2.289858102798462, site loss 0.04791535809636116
Iteration 51, loss 2.353969097137451, site loss 0.04706968367099762
Iteration 52, loss 2.3280959129333496, site loss 0.05155808478593826
Iteration 53, loss 2.288843870162964, site loss 0.04954783245921135
Iteration 54, loss 2.2811927795410156, site loss 0.04836123064160347
Iteration 55, loss 2.256629467010498, site loss 0.05034869909286499
Iteration 56, loss 2.222917318344116, site loss 0.046065278351306915
Iteration 57, loss 2.3089771270751953, site loss 0.045864537358284
Iteration 58, loss 2.392643928527832, site loss 0.0485374741256237
Iteration 59, loss 2.336228847503662, site loss 0.04936140030622482
Iteration 60, loss 2.3879141807556152, site loss 0.04640558362007141
Iteration 61, loss 2.335653066635132, site loss 0.04825736954808235
Iteration 62, loss 2.30411434173584, site loss 0.051737990230321884
Iteration 63, loss 2.3474669456481934, site loss 0.05452428758144379
Iteration 64, loss 2.370757579803467, site loss 0.05126179754734039
Iteration 65, loss 2.34151029586792, site loss 0.05109040439128876
Iteration 66, loss 2.3448667526245117, site loss 0.05059617757797241
Iteration 67, loss 2.350905418395996, site loss 0.05248164013028145
Iteration 68, loss 2.3516414165496826, site loss 0.0492868535220623
Iteration 69, loss 2.3850760459899902, site loss 0.05373639613389969
Iteration 70, loss 2.344118118286133, site loss 0.05094342678785324
Iteration 71, loss 2.3873977661132812, site loss 0.0538657084107399
Iteration 72, loss 2.380706310272217, site loss 0.04990560561418533
Iteration 73, loss 2.2710845470428467, site loss 0.04772711917757988
Iteration 74, loss 2.3479230403900146, site loss 0.049788184463977814
Iteration 75, loss 2.305401086807251, site loss 0.05122481286525726
Iteration 76, loss 2.326603412628174, site loss 0.048396021127700806
Iteration 77, loss 2.3099191188812256, site loss 0.0439685694873333
Iteration 78, loss 2.3403515815734863, site loss 0.05000850930809975
Iteration 79, loss 2.311244487762451, site loss 0.04775794968008995
Iteration 80, loss 2.30788516998291, site loss 0.04888540506362915
Iteration 81, loss 2.2318830490112305, site loss 0.04797264561057091
Iteration 82, loss 2.2885115146636963, site loss 0.04467031732201576
Iteration 83, loss 2.288003444671631, site loss 0.046164289116859436
Iteration 84, loss 2.315011978149414, site loss 0.047916240990161896
Iteration 85, loss 2.3016772270202637, site loss 0.04795869439840317
Iteration 86, loss 2.357128620147705, site loss 0.04788530617952347
Iteration 87, loss 2.2985172271728516, site loss 0.04592831805348396
Iteration 88, loss 2.3367295265197754, site loss 0.05161742866039276
Iteration 89, loss 2.3618597984313965, site loss 0.05143502354621887
Iteration 90, loss 2.2679646015167236, site loss 0.04521310329437256
Iteration 91, loss 2.2908499240875244, site loss 0.04727805405855179
Iteration 92, loss 2.3170759677886963, site loss 0.0436713770031929
Iteration 93, loss 2.2679643630981445, site loss 0.05185471475124359
Iteration 94, loss 2.329726457595825, site loss 0.05135480314493179
Iteration 95, loss 2.288816452026367, site loss 0.051350273191928864
Iteration 96, loss 2.386319160461426, site loss 0.04901958629488945
Iteration 97, loss 2.306131601333618, site loss 0.04975742846727371
Iteration 98, loss 2.307852029800415, site loss 0.04705848544836044
Iteration 99, loss 2.3377580642700195, site loss 0.05186452716588974
Iteration 100, loss 2.349092960357666, site loss 0.04677870124578476
Iteration 101, loss 2.312201499938965, site loss 0.04529689997434616
Iteration 102, loss 2.283090829849243, site loss 0.04496851563453674
Iteration 103, loss 2.31581974029541, site loss 0.04811173677444458
Iteration 104, loss 2.297015428543091, site loss 0.04636043310165405
Iteration 105, loss 2.2791049480438232, site loss 0.04841158539056778
Iteration 106, loss 2.3245668411254883, site loss 0.04517525061964989
Iteration 107, loss 2.2984085083007812, site loss 0.04538706690073013
Iteration 108, loss 2.328808307647705, site loss 0.045207664370536804
Iteration 109, loss 2.762220859527588, site loss 0.09855464100837708
Iteration 110, loss 2.3736815452575684, site loss 0.04599868506193161
Iteration 111, loss 2.3177130222320557, site loss 0.043116431683301926
Iteration 112, loss 2.313514232635498, site loss 0.045047808438539505
Iteration 113, loss 2.3028926849365234, site loss 0.04865922033786774
Iteration 114, loss 2.2920401096343994, site loss 0.04606132209300995
Iteration 115, loss 2.3267266750335693, site loss 0.04616504907608032
Iteration 116, loss 2.3414177894592285, site loss 0.043643102049827576
Iteration 117, loss 2.3392252922058105, site loss 0.04506536200642586
Iteration 118, loss 2.265888214111328, site loss 0.04611191898584366
Iteration 119, loss 2.2770872116088867, site loss 0.04026910662651062
Iteration 120, loss 2.2832465171813965, site loss 0.04420303925871849
Iteration 121, loss 2.2600371837615967, site loss 0.04353818669915199
Iteration 122, loss 2.284235954284668, site loss 0.044073693454265594
Iteration 123, loss 2.2942724227905273, site loss 0.04151230305433273
Iteration 124, loss 2.275343179702759, site loss 0.0429307296872139
Iteration 125, loss 2.203132390975952, site loss 0.042775917798280716
Iteration 126, loss 2.243992328643799, site loss 0.04095624014735222
Iteration 127, loss 2.239595413208008, site loss 0.04195934534072876
Iteration 128, loss 2.2489356994628906, site loss 0.038895778357982635
Iteration 129, loss 2.2707648277282715, site loss 0.04205241799354553
Iteration 130, loss 2.2470321655273438, site loss 0.040987707674503326
Iteration 131, loss 2.2118916511535645, site loss 0.040002502501010895
Iteration 132, loss 2.2203192710876465, site loss 0.03990761563181877
Iteration 133, loss 2.302983522415161, site loss 0.039722882211208344
Iteration 134, loss 2.239205837249756, site loss 0.0393727570772171
Iteration 135, loss 2.2561593055725098, site loss 0.03651666268706322
Iteration 136, loss 2.158630847930908, site loss 0.03934546560049057
Iteration 137, loss 2.2049810886383057, site loss 0.039126165211200714
Iteration 138, loss 2.1938912868499756, site loss 0.03738170489668846
Iteration 139, loss 2.197793483734131, site loss 0.03891643509268761
Iteration 140, loss 2.2070505619049072, site loss 0.038228102028369904
Iteration 141, loss 2.2045187950134277, site loss 0.03380352258682251
Iteration 142, loss 2.18717622756958, site loss 0.0361371673643589
Iteration 143, loss 2.1719846725463867, site loss 0.03571374714374542
Iteration 144, loss 2.225651741027832, site loss 0.03783968463540077
Iteration 145, loss 2.156846523284912, site loss 0.03507768362760544
Iteration 146, loss 2.297548294067383, site loss 0.03894168138504028
Iteration 147, loss 2.156437873840332, site loss 0.035072170197963715
Iteration 148, loss 2.198540687561035, site loss 0.03636162728071213
Iteration 149, loss 2.18630051612854, site loss 0.03413045033812523
Iteration 150, loss 2.2077174186706543, site loss 0.036990705877542496
Iteration 151, loss 2.173008441925049, site loss 0.031034039333462715
Iteration 152, loss 2.1880810260772705, site loss 0.03344196826219559
Iteration 153, loss 2.2125871181488037, site loss 0.03648359328508377
Iteration 154, loss 2.1724820137023926, site loss 0.033241335302591324
Iteration 155, loss 2.1475648880004883, site loss 0.03290345519781113
Iteration 156, loss 2.1596479415893555, site loss 0.03363809734582901
Iteration 157, loss 2.171658992767334, site loss 0.033254656940698624
Iteration 158, loss 2.1766090393066406, site loss 0.032342880964279175
Iteration 159, loss 2.1735448837280273, site loss 0.03337131440639496
Iteration 160, loss 2.227450132369995, site loss 0.03517428785562515
Iteration 161, loss 2.1710610389709473, site loss 0.03445952758193016
Iteration 162, loss 2.1749868392944336, site loss 0.035019975155591965
Iteration 163, loss 2.154679775238037, site loss 0.0302771907299757
Iteration 164, loss 2.139716863632202, site loss 0.031721875071525574
Iteration 165, loss 2.1086037158966064, site loss 0.031045446172356606
Iteration 166, loss 2.1397862434387207, site loss 0.030027372762560844
Iteration 167, loss 2.158425807952881, site loss 0.02958493120968342
Iteration 168, loss 2.166295051574707, site loss 0.031463153660297394
Iteration 169, loss 2.092376947402954, site loss 0.0332779735326767
Iteration 170, loss 2.096503973007202, site loss 0.030491501092910767
Iteration 171, loss 2.0461812019348145, site loss 0.029442038387060165
Iteration 172, loss 1.9677752256393433, site loss 0.027470335364341736
Iteration 173, loss 2.131028175354004, site loss 0.028334621340036392
Iteration 174, loss 2.0852668285369873, site loss 0.03011823631823063
Iteration 175, loss 2.057816743850708, site loss 0.02815883234143257
Iteration 176, loss 2.106987714767456, site loss 0.029495887458324432
Iteration 177, loss 2.093123197555542, site loss 0.027416527271270752
Iteration 178, loss 2.0656251907348633, site loss 0.026471029967069626
Iteration 179, loss 2.0687108039855957, site loss 0.027774274349212646
Iteration 180, loss 2.0918941497802734, site loss 0.03012227639555931
Iteration 181, loss 1.9975271224975586, site loss 0.026266271248459816
Iteration 182, loss 2.047243118286133, site loss 0.02696264162659645
Iteration 183, loss 2.0789265632629395, site loss 0.029749013483524323
Iteration 184, loss 2.0564186573028564, site loss 0.02610054984688759
Iteration 185, loss 2.0982985496520996, site loss 0.030395474284887314
Iteration 186, loss 2.041752815246582, site loss 0.027808241546154022
Iteration 187, loss 1.9988781213760376, site loss 0.027559787034988403
Iteration 188, loss 1.9616843461990356, site loss 0.0295296348631382
Iteration 189, loss 2.0259764194488525, site loss 0.02846643142402172
Iteration 190, loss 2.0511746406555176, site loss 0.02758377604186535
Iteration 191, loss 1.9972641468048096, site loss 0.027716336771845818
Iteration 192, loss 2.054349422454834, site loss 0.02730453759431839
Iteration 193, loss 2.0587754249572754, site loss 0.029563628137111664
Iteration 194, loss 2.0927886962890625, site loss 0.03248409181833267
Iteration 195, loss 2.0345749855041504, site loss 0.0262705497443676
Iteration 196, loss 2.0665767192840576, site loss 0.02834359183907509
Iteration 197, loss 2.1022300720214844, site loss 0.03256131708621979
Iteration 198, loss 2.691364288330078, site loss 0.0692911148071289
Iteration 199, loss 2.0144340991973877, site loss 0.02909347042441368
Iteration 200, loss 2.05041241645813, site loss 0.028283292427659035
Iteration 201, loss 2.0498263835906982, site loss 0.03019593469798565
Iteration 202, loss 2.037351608276367, site loss 0.029766147956252098
Iteration 203, loss 1.9912440776824951, site loss 0.029624763876199722
Iteration 204, loss 2.078348159790039, site loss 0.02789376676082611
Iteration 205, loss 1.9554564952850342, site loss 0.026325751096010208
Iteration 206, loss 1.9807982444763184, site loss 0.02756754495203495
Iteration 207, loss 1.918881893157959, site loss 0.02645072713494301
Iteration 208, loss 1.9310405254364014, site loss 0.02911122888326645
Iteration 209, loss 1.8951553106307983, site loss 0.02430812269449234
Iteration 210, loss 1.963040828704834, site loss 0.02718295529484749
Iteration 211, loss 1.9061697721481323, site loss 0.02398037165403366
Iteration 212, loss 1.9240326881408691, site loss 0.02744806930422783
Iteration 213, loss 1.940713882446289, site loss 0.027610156685113907
Iteration 214, loss 1.8984975814819336, site loss 0.02526700124144554
Iteration 215, loss 1.8943548202514648, site loss 0.023596739396452904
Iteration 216, loss 1.9006749391555786, site loss 0.02739574760198593
Iteration 217, loss 1.9023640155792236, site loss 0.024896923452615738
Iteration 218, loss 1.889390468597412, site loss 0.0260984618216753
Iteration 219, loss 1.7977347373962402, site loss 0.025327961891889572
Iteration 220, loss 1.8987865447998047, site loss 0.02379636839032173
Iteration 221, loss 1.88013756275177, site loss 0.024794835597276688
Iteration 222, loss 1.8916809558868408, site loss 0.024451419711112976
Iteration 223, loss 1.9149842262268066, site loss 0.027364157140254974
Iteration 224, loss 1.8509682416915894, site loss 0.023978834971785545
Iteration 225, loss 1.8900771141052246, site loss 0.02628924697637558
Iteration 226, loss 1.8444956541061401, site loss 0.02291659638285637
Iteration 227, loss 1.8383715152740479, site loss 0.025284649804234505
Iteration 228, loss 1.8787117004394531, site loss 0.026750609278678894
Iteration 229, loss 1.8193719387054443, site loss 0.022480104118585587
Iteration 230, loss 1.8064565658569336, site loss 0.023517603054642677
Iteration 231, loss 1.812934160232544, site loss 0.024705497547984123
Iteration 232, loss 1.8638699054718018, site loss 0.02502450719475746
Iteration 233, loss 1.8095383644104004, site loss 0.027959521859884262
Iteration 234, loss 1.8203299045562744, site loss 0.02361362800002098
Iteration 235, loss 1.834463119506836, site loss 0.02440434694290161
Iteration 236, loss 1.8037358522415161, site loss 0.02260555326938629
Iteration 237, loss 1.8544667959213257, site loss 0.023464292287826538
Iteration 238, loss 1.8032408952713013, site loss 0.023555392399430275
Iteration 239, loss 1.8169643878936768, site loss 0.024714741855859756
Iteration 240, loss 1.8454713821411133, site loss 0.025132492184638977
Iteration 241, loss 1.9553940296173096, site loss 0.027177277952432632
Iteration 242, loss 1.8411457538604736, site loss 0.023171884939074516
Iteration 243, loss 1.8616070747375488, site loss 0.023699745535850525
Iteration 244, loss 1.8648054599761963, site loss 0.025211520493030548
Iteration 245, loss 1.9186749458312988, site loss 0.026770872995257378
Iteration 246, loss 1.8240119218826294, site loss 0.022106854245066643
Iteration 247, loss 1.8028311729431152, site loss 0.02346479892730713
Iteration 248, loss 1.8290786743164062, site loss 0.02324300818145275
Iteration 249, loss 1.8150756359100342, site loss 0.023571869358420372
Iteration 250, loss 1.7875107526779175, site loss 0.021045565605163574
Iteration 251, loss 1.7843120098114014, site loss 0.021609732881188393
Iteration 252, loss 1.8076566457748413, site loss 0.023497965186834335
Iteration 253, loss 1.8510291576385498, site loss 0.024324290454387665
Iteration 254, loss 1.7964603900909424, site loss 0.021823221817612648
Iteration 255, loss 1.797623872756958, site loss 0.020473666489124298
Iteration 256, loss 1.7594046592712402, site loss 0.02281486615538597
Iteration 257, loss 1.804764986038208, site loss 0.021944545209407806
Iteration 258, loss 1.7857133150100708, site loss 0.02072497271001339
Iteration 259, loss 1.8135716915130615, site loss 0.024143580347299576
Iteration 260, loss 1.7186551094055176, site loss 0.019575458019971848
Iteration 261, loss 1.757857322692871, site loss 0.019589148461818695
Iteration 262, loss 1.7196400165557861, site loss 0.01876327022910118
Iteration 263, loss 1.8544567823410034, site loss 0.024415399879217148
Iteration 264, loss 1.8343327045440674, site loss 0.02066020667552948
Iteration 265, loss 1.7187317609786987, site loss 0.020265892148017883
Iteration 266, loss 1.8335585594177246, site loss 0.020985541865229607
Iteration 267, loss 1.7932708263397217, site loss 0.021972820162773132
Iteration 268, loss 1.7591288089752197, site loss 0.02398000657558441
Iteration 269, loss 1.7484418153762817, site loss 0.021465588361024857
Iteration 270, loss 1.7661707401275635, site loss 0.01985304430127144
Iteration 271, loss 1.7672755718231201, site loss 0.019120166078209877
Iteration 272, loss 1.7314059734344482, site loss 0.01904023066163063
Iteration 273, loss 1.719226360321045, site loss 0.018849384039640427
Iteration 274, loss 1.7097837924957275, site loss 0.0198859553784132
Iteration 275, loss 1.775900959968567, site loss 0.020578453317284584
Iteration 276, loss 1.7641007900238037, site loss 0.021568814292550087
Iteration 277, loss 1.7499570846557617, site loss 0.021082529798150063
Iteration 278, loss 1.7242579460144043, site loss 0.01895367167890072
Iteration 279, loss 1.8236889839172363, site loss 0.02087082713842392
Iteration 280, loss 1.8592555522918701, site loss 0.022976096719503403
Iteration 281, loss 1.7519831657409668, site loss 0.020567085593938828
Iteration 282, loss 1.7489914894104004, site loss 0.020182624459266663
Iteration 283, loss 1.7456501722335815, site loss 0.02169955149292946
Iteration 284, loss 1.8137482404708862, site loss 0.020687170326709747
Iteration 285, loss 1.7385060787200928, site loss 0.01982048898935318
Iteration 286, loss 1.8272098302841187, site loss 0.021858811378479004
Iteration 287, loss 1.7233197689056396, site loss 0.022756200283765793
Iteration 288, loss 1.8053689002990723, site loss 0.021964311599731445
Iteration 289, loss 1.8097145557403564, site loss 0.023105312138795853
Iteration 290, loss 1.7882106304168701, site loss 0.024478457868099213
Iteration 291, loss 1.7546846866607666, site loss 0.022775810211896896
Iteration 292, loss 1.8240468502044678, site loss 0.02089262753725052
Iteration 293, loss 1.7834479808807373, site loss 0.021646002307534218
Iteration 294, loss 1.7721785306930542, site loss 0.020118094980716705
Iteration 295, loss 1.8551335334777832, site loss 0.023860709741711617
Iteration 296, loss 1.7912683486938477, site loss 0.020338259637355804
Iteration 297, loss 1.8137879371643066, site loss 0.020391371101140976
Iteration 298, loss 1.792252779006958, site loss 0.02237119898200035
Iteration 299, loss 1.793220043182373, site loss 0.023711681365966797
Iteration 300, loss 1.85209059715271, site loss 0.021310053765773773
Iteration 301, loss 1.7805006504058838, site loss 0.01970973052084446
Iteration 302, loss 1.7073721885681152, site loss 0.019978929311037064
Iteration 303, loss 1.8056567907333374, site loss 0.021326087415218353
Iteration 304, loss 1.6999869346618652, site loss 0.020284241065382957
Iteration 305, loss 1.7645268440246582, site loss 0.02031782828271389
Iteration 306, loss 1.8085708618164062, site loss 0.020537611097097397
Iteration 307, loss 1.8036737442016602, site loss 0.02153969183564186
Iteration 308, loss 1.793576717376709, site loss 0.020082799717783928
Iteration 309, loss 1.7595480680465698, site loss 0.019970087334513664
Iteration 310, loss 1.8342164754867554, site loss 0.018420012667775154
Iteration 311, loss 1.7676794528961182, site loss 0.019718220457434654
Iteration 312, loss 1.7569581270217896, site loss 0.020500103011727333
Iteration 313, loss 1.778528094291687, site loss 0.022707294672727585
Iteration 314, loss 2.3783645629882812, site loss 0.048766084015369415
Iteration 315, loss 1.8164118528366089, site loss 0.02007606439292431
Iteration 316, loss 1.804402232170105, site loss 0.01869776099920273
Iteration 317, loss 1.7496778964996338, site loss 0.018742330372333527
Iteration 318, loss 1.759518027305603, site loss 0.018848074600100517
Iteration 319, loss 1.8109135627746582, site loss 0.020826861262321472
Iteration 320, loss 1.7974019050598145, site loss 0.01929713413119316
Iteration 321, loss 1.8187055587768555, site loss 0.0189763605594635
Iteration 322, loss 1.7756848335266113, site loss 0.017479803413152695
Iteration 323, loss 1.8358006477355957, site loss 0.020046569406986237
Iteration 324, loss 1.7873896360397339, site loss 0.01990041881799698
Iteration 325, loss 1.7422659397125244, site loss 0.02002778835594654
Iteration 326, loss 1.756937026977539, site loss 0.018391994759440422
Iteration 327, loss 1.8337551355361938, site loss 0.019368235021829605
Iteration 328, loss 1.8155105113983154, site loss 0.019188523292541504
Iteration 329, loss 1.7785248756408691, site loss 0.019876260310411453
Iteration 330, loss 1.7810883522033691, site loss 0.019385356456041336
Iteration 331, loss 1.7573837041854858, site loss 0.01816272735595703
Iteration 332, loss 1.7901239395141602, site loss 0.02021339163184166
Iteration 333, loss 1.7302005290985107, site loss 0.019857436418533325
Iteration 334, loss 1.6658350229263306, site loss 0.017795126885175705
Iteration 335, loss 1.7225079536437988, site loss 0.018837550655007362
Iteration 336, loss 1.681444764137268, site loss 0.015920476987957954
Iteration 337, loss 1.7308450937271118, site loss 0.01908552274107933
Iteration 338, loss 1.7255609035491943, site loss 0.019757473841309547
Iteration 339, loss 1.7697633504867554, site loss 0.022775346413254738
Iteration 340, loss 1.7421307563781738, site loss 0.016598334535956383
Iteration 341, loss 1.6666038036346436, site loss 0.01681627705693245
Iteration 342, loss 1.5722098350524902, site loss 0.015480373054742813
Iteration 343, loss 1.7575434446334839, site loss 0.017981689423322678
Iteration 344, loss 1.7296385765075684, site loss 0.01807510480284691
Iteration 345, loss 1.7140235900878906, site loss 0.015304618515074253
Iteration 346, loss 1.752657175064087, site loss 0.015297813341021538
Iteration 347, loss 1.6922008991241455, site loss 0.018265914171934128
Iteration 348, loss 1.7229280471801758, site loss 0.01768770068883896
Iteration 349, loss 1.8102573156356812, site loss 0.01709018461406231
Iteration 350, loss 1.7326757907867432, site loss 0.015373600646853447
Iteration 351, loss 1.7003966569900513, site loss 0.01724115014076233
Iteration 352, loss 1.7144427299499512, site loss 0.01665361411869526
Iteration 353, loss 1.7388142347335815, site loss 0.015994563698768616
Iteration 354, loss 1.655626654624939, site loss 0.014264426194131374
Iteration 355, loss 1.7003192901611328, site loss 0.01691102609038353
Iteration 356, loss 1.6565721035003662, site loss 0.016775794327259064
Iteration 357, loss 1.7862499952316284, site loss 0.018307387828826904
Iteration 358, loss 1.7251343727111816, site loss 0.015830088406801224
Iteration 359, loss 1.831592321395874, site loss 0.020898528397083282
Iteration 360, loss 1.7043099403381348, site loss 0.01640733703970909
Iteration 361, loss 1.7217020988464355, site loss 0.016594145447015762
Iteration 362, loss 1.7951104640960693, site loss 0.016695378348231316
Iteration 363, loss 1.6603962182998657, site loss 0.01433052308857441
Iteration 364, loss 1.7813447713851929, site loss 0.018316112458705902
Iteration 365, loss 1.6057851314544678, site loss 0.013638629578053951
Iteration 366, loss 1.6713335514068604, site loss 0.014029320329427719
Iteration 367, loss 1.6715664863586426, site loss 0.01364901289343834
Iteration 368, loss 1.6526294946670532, site loss 0.015053590759634972
Iteration 369, loss 1.6350868940353394, site loss 0.012287374585866928
Iteration 370, loss 1.6886703968048096, site loss 0.014260931871831417
Iteration 371, loss 1.722640872001648, site loss 0.014779709279537201
Iteration 372, loss 1.7085191011428833, site loss 0.0129756610840559
Iteration 373, loss 1.6281696557998657, site loss 0.017772844061255455
Iteration 374, loss 1.7508764266967773, site loss 0.016220275312662125
Iteration 375, loss 1.6955206394195557, site loss 0.0150340236723423
Iteration 376, loss 1.7104140520095825, site loss 0.016508355736732483
Iteration 377, loss 1.6705868244171143, site loss 0.015268353745341301
Iteration 378, loss 1.6822785139083862, site loss 0.016707655042409897
Iteration 379, loss 1.6615697145462036, site loss 0.01309872791171074
Iteration 380, loss 1.736624002456665, site loss 0.01599961705505848
Iteration 381, loss 1.5994207859039307, site loss 0.013781493529677391
Iteration 382, loss 1.7199931144714355, site loss 0.01648581586778164
Iteration 383, loss 1.7079799175262451, site loss 0.01641087420284748
Iteration 384, loss 1.6490049362182617, site loss 0.015390438959002495
Iteration 385, loss 1.6518876552581787, site loss 0.015734901651740074
Iteration 386, loss 1.6799789667129517, site loss 0.017042048275470734
Iteration 387, loss 1.626498818397522, site loss 0.01381116732954979
Iteration 388, loss 1.7596168518066406, site loss 0.015233375132083893
Iteration 389, loss 1.7106873989105225, site loss 0.01827862486243248
Iteration 390, loss 1.7818324565887451, site loss 0.017682209610939026
Iteration 391, loss 1.600601315498352, site loss 0.017199283465743065
Iteration 392, loss 1.63974928855896, site loss 0.015790682286024094
Iteration 393, loss 1.5958714485168457, site loss 0.014661322347819805
Iteration 394, loss 1.7356419563293457, site loss 0.016991417855024338
Iteration 395, loss 1.664595603942871, site loss 0.01611894741654396
Iteration 396, loss 1.691158413887024, site loss 0.017406443133950233
Iteration 397, loss 1.7021230459213257, site loss 0.01528872363269329
Iteration 398, loss 1.5946755409240723, site loss 0.014103665947914124
Iteration 399, loss 1.625458002090454, site loss 0.016323495656251907
Iteration 400, loss 2.1106467247009277, site loss 0.031217627227306366
Iteration 401, loss 1.5847456455230713, site loss 0.015004567801952362
Iteration 402, loss 1.6464537382125854, site loss 0.016862666234374046
Iteration 403, loss 1.6695377826690674, site loss 0.01653442159295082
Iteration 404, loss 1.64231538772583, site loss 0.014994814991950989
Iteration 405, loss 1.6317307949066162, site loss 0.014917460270226002
Iteration 406, loss 1.6545929908752441, site loss 0.015593897551298141
Iteration 407, loss 1.6525352001190186, site loss 0.016471438109874725
Iteration 408, loss 1.616384506225586, site loss 0.015156349167227745
Iteration 409, loss 1.6363286972045898, site loss 0.015072834677994251
Iteration 410, loss 1.6118582487106323, site loss 0.015073946677148342
Iteration 411, loss 1.605467677116394, site loss 0.016624815762043
Iteration 412, loss 1.65658700466156, site loss 0.014141783118247986
Iteration 413, loss 1.6265332698822021, site loss 0.014151202514767647
Iteration 414, loss 1.5971665382385254, site loss 0.01638350635766983
Iteration 415, loss 1.536137580871582, site loss 0.012238407507538795
Iteration 416, loss 1.6834759712219238, site loss 0.01556318812072277
Iteration 417, loss 1.552520513534546, site loss 0.017069116234779358
Iteration 418, loss 1.678560733795166, site loss 0.01746012270450592
Iteration 419, loss 1.6525506973266602, site loss 0.014920460060238838
Iteration 420, loss 1.6049922704696655, site loss 0.015101372264325619
Iteration 421, loss 1.5728273391723633, site loss 0.015562765300273895
Iteration 422, loss 1.5757055282592773, site loss 0.01523529551923275
Iteration 423, loss 1.621255874633789, site loss 0.016742849722504616
Iteration 424, loss 1.6954078674316406, site loss 0.017952710390090942
Iteration 425, loss 1.620244026184082, site loss 0.014287134632468224
Iteration 426, loss 1.5842255353927612, site loss 0.01420636661350727
Iteration 427, loss 1.5704452991485596, site loss 0.013716867193579674
Iteration 428, loss 1.642613172531128, site loss 0.016021648421883583
Iteration 429, loss 1.5612702369689941, site loss 0.015231712721288204
Iteration 430, loss 1.5969703197479248, site loss 0.015660451725125313
Iteration 431, loss 1.559814691543579, site loss 0.015151714906096458
Iteration 432, loss 1.572420597076416, site loss 0.014339083805680275
Iteration 433, loss 1.5024915933609009, site loss 0.012436467222869396
Iteration 434, loss 1.5201126337051392, site loss 0.01259267795830965
Iteration 435, loss 1.6444231271743774, site loss 0.01654857024550438
Iteration 436, loss 1.5795749425888062, site loss 0.013680785894393921
Iteration 437, loss 1.5805277824401855, site loss 0.01376485824584961
Iteration 438, loss 1.5765669345855713, site loss 0.013370676897466183
Iteration 439, loss 1.598112940788269, site loss 0.01418222300708294
Iteration 440, loss 1.632893681526184, site loss 0.013264160603284836
Iteration 441, loss 1.6591423749923706, site loss 0.01826595328748226
Iteration 442, loss 1.5538713932037354, site loss 0.013523139990866184
Iteration 443, loss 1.6024775505065918, site loss 0.015695547685027122
Iteration 444, loss 1.5329372882843018, site loss 0.01337362639605999
Iteration 445, loss 1.5087532997131348, site loss 0.01465882733464241
Iteration 446, loss 1.613097906112671, site loss 0.015928620472550392
Iteration 447, loss 1.518216848373413, site loss 0.016323884949088097
Iteration 448, loss 1.5038747787475586, site loss 0.013875512406229973
Iteration 449, loss 1.470226526260376, site loss 0.011347511783242226
Iteration 450, loss 1.5845921039581299, site loss 0.013563502579927444
Iteration 451, loss 1.5160821676254272, site loss 0.01395316980779171
Iteration 452, loss 1.561658501625061, site loss 0.014173381961882114
Iteration 453, loss 1.5469270944595337, site loss 0.014100324362516403
Iteration 454, loss 1.4879975318908691, site loss 0.012651345692574978
Iteration 455, loss 1.5389196872711182, site loss 0.01653650775551796
Iteration 456, loss 1.5078277587890625, site loss 0.01218031533062458
Iteration 457, loss 1.509292721748352, site loss 0.01187562569975853
Iteration 458, loss 1.4256916046142578, site loss 0.010954968631267548
Iteration 459, loss 1.6513711214065552, site loss 0.012982401996850967
Iteration 460, loss 1.4699296951293945, site loss 0.010738471522927284
Iteration 461, loss 1.4787899255752563, site loss 0.011914493516087532
Iteration 462, loss 1.4700305461883545, site loss 0.013148786500096321
Iteration 463, loss 1.5333280563354492, site loss 0.01522701233625412
Iteration 464, loss 1.5824449062347412, site loss 0.013471913523972034
Iteration 465, loss 1.5348451137542725, site loss 0.0148480124771595
Iteration 466, loss 1.5740457773208618, site loss 0.014683494344353676
Iteration 467, loss 1.5205168724060059, site loss 0.012378064915537834
Iteration 468, loss 1.5264482498168945, site loss 0.012737066484987736
Iteration 469, loss 1.4738194942474365, site loss 0.015541131608188152
Iteration 470, loss 1.4200187921524048, site loss 0.011447027325630188
Iteration 471, loss 1.5017271041870117, site loss 0.010937870480120182
Iteration 472, loss 1.4659771919250488, site loss 0.011032290756702423
Iteration 473, loss 1.6245307922363281, site loss 0.013169982470571995
Iteration 474, loss 1.5627894401550293, site loss 0.013345954939723015
Iteration 475, loss 1.470712423324585, site loss 0.012714224867522717
Iteration 476, loss 1.4624524116516113, site loss 0.012500688433647156
Iteration 477, loss 1.5131551027297974, site loss 0.01285509578883648
Iteration 478, loss 1.4578993320465088, site loss 0.010417432524263859
Iteration 479, loss 1.5342870950698853, site loss 0.012220988050103188
Iteration 480, loss 1.5417108535766602, site loss 0.013228626921772957
Iteration 481, loss 1.4539926052093506, site loss 0.013879540376365185
Iteration 482, loss 1.573158621788025, site loss 0.01724383980035782
Iteration 483, loss 1.5264036655426025, site loss 0.013782615773379803
Iteration 484, loss 1.6347575187683105, site loss 0.013753503561019897
Iteration 485, loss 1.4081470966339111, site loss 0.010076410137116909
Iteration 486, loss 1.5131171941757202, site loss 0.012952208518981934
Iteration 487, loss 1.5143723487854004, site loss 0.012575825676321983
Iteration 488, loss 1.5231685638427734, site loss 0.01315433531999588
Iteration 489, loss 1.4808995723724365, site loss 0.013247751630842686
Iteration 490, loss 1.3880116939544678, site loss 0.009943035431206226
Iteration 491, loss 1.5523968935012817, site loss 0.012655554339289665
Iteration 492, loss 1.544135332107544, site loss 0.01498014573007822
Iteration 493, loss 1.4135258197784424, site loss 0.011933458037674427
Iteration 494, loss 1.5874062776565552, site loss 0.012893332168459892
Iteration 495, loss 1.5272444486618042, site loss 0.01131881307810545
Iteration 496, loss 1.5520268678665161, site loss 0.01265028864145279
Iteration 497, loss 1.525539517402649, site loss 0.013089848682284355
Iteration 498, loss 1.4888889789581299, site loss 0.012373523786664009
Iteration 499, loss 1.4678316116333008, site loss 0.013598434627056122
Iteration 500, loss 1.448601484298706, site loss 0.011087251827120781
Iteration 501, loss 1.4969146251678467, site loss 0.011702232994139194
Iteration 502, loss 1.530099630355835, site loss 0.013543291948735714
Iteration 503, loss 1.5409297943115234, site loss 0.012594850733876228
Iteration 504, loss 1.5642038583755493, site loss 0.015704989433288574
Iteration 505, loss 1.5199801921844482, site loss 0.012881873175501823
Iteration 506, loss 1.449970006942749, site loss 0.011943747289478779
Iteration 507, loss 1.570688009262085, site loss 0.012838538736104965
Iteration 508, loss 1.5419869422912598, site loss 0.01101887971162796
Iteration 509, loss 1.511919617652893, site loss 0.011432245373725891
Iteration 510, loss 1.5059468746185303, site loss 0.012838779017329216
Iteration 511, loss 1.5217949151992798, site loss 0.011337481439113617
Iteration 512, loss 1.4641180038452148, site loss 0.012430768460035324
Iteration 513, loss 1.5156738758087158, site loss 0.016139056533575058
Iteration 514, loss 1.4661221504211426, site loss 0.012132803909480572
Iteration 515, loss 1.5373274087905884, site loss 0.011568953283131123
Iteration 516, loss 1.4094024896621704, site loss 0.011452323757112026
Iteration 517, loss 1.4670342206954956, site loss 0.012894849292933941
Iteration 518, loss 1.4951039552688599, site loss 0.011621678248047829
Iteration 519, loss 1.5254786014556885, site loss 0.012529154308140278
Iteration 520, loss 1.5084691047668457, site loss 0.010955962352454662
Iteration 521, loss 1.5008327960968018, site loss 0.012549908831715584
Iteration 522, loss 1.4958562850952148, site loss 0.011669999919831753
Iteration 523, loss 1.4777348041534424, site loss 0.011532003991305828
Iteration 524, loss 1.5465550422668457, site loss 0.012364467605948448
Iteration 525, loss 2.0477848052978516, site loss 0.028652667999267578
Iteration 526, loss 1.4844672679901123, site loss 0.010666172951459885
Iteration 527, loss 1.4794574975967407, site loss 0.013208456337451935
Iteration 528, loss 1.6431519985198975, site loss 0.013109955936670303
Iteration 529, loss 1.4957773685455322, site loss 0.012646113522350788
Iteration 530, loss 1.5699436664581299, site loss 0.010934999212622643
Iteration 531, loss 1.5532013177871704, site loss 0.014881646260619164
Iteration 532, loss 1.5715723037719727, site loss 0.012135202996432781
Iteration 533, loss 1.6441386938095093, site loss 0.014832359738647938
Iteration 534, loss 1.5189976692199707, site loss 0.013753384351730347
Iteration 535, loss 1.5163719654083252, site loss 0.01135803759098053
Iteration 536, loss 1.5817511081695557, site loss 0.012525000609457493
Iteration 537, loss 1.5448286533355713, site loss 0.011653773486614227
Iteration 538, loss 1.561262607574463, site loss 0.011052077636122704
Iteration 539, loss 1.4123789072036743, site loss 0.01157151535153389
Iteration 540, loss 1.5311529636383057, site loss 0.011117976158857346
Iteration 541, loss 1.6238394975662231, site loss 0.012462601996958256
Iteration 542, loss 1.4391263723373413, site loss 0.010457994416356087
Iteration 543, loss 1.5426406860351562, site loss 0.012365623377263546
Iteration 544, loss 1.4966109991073608, site loss 0.011226627975702286
Iteration 545, loss 1.5437504053115845, site loss 0.01241566613316536
Iteration 546, loss 1.5346190929412842, site loss 0.012177689000964165
Iteration 547, loss 1.5287871360778809, site loss 0.010816243477165699
Iteration 548, loss 1.4295179843902588, site loss 0.010490719228982925
Iteration 549, loss 1.5421159267425537, site loss 0.011019629426300526
Iteration 550, loss 1.5197782516479492, site loss 0.010980493389070034
Iteration 551, loss 1.5428740978240967, site loss 0.010569607838988304
Iteration 552, loss 1.4647279977798462, site loss 0.012393053621053696
Iteration 553, loss 1.5684411525726318, site loss 0.010019620880484581
Iteration 554, loss 1.5030608177185059, site loss 0.011053724214434624
Iteration 555, loss 1.4933981895446777, site loss 0.010893722996115685
Iteration 556, loss 1.5150032043457031, site loss 0.011267215944826603
Iteration 557, loss 1.5247673988342285, site loss 0.010659356601536274
Iteration 558, loss 1.5745340585708618, site loss 0.0132701825350523
Iteration 559, loss 1.413495659828186, site loss 0.009404590353369713
Iteration 560, loss 1.5193367004394531, site loss 0.010893548838794231
Iteration 561, loss 1.4830734729766846, site loss 0.011064196936786175
Iteration 562, loss 1.4119709730148315, site loss 0.010569807142019272
Iteration 563, loss 1.4438947439193726, site loss 0.009807473048567772
Iteration 564, loss 1.458115577697754, site loss 0.013087663799524307
Iteration 565, loss 1.5268553495407104, site loss 0.010694518685340881
Iteration 566, loss 1.3537929058074951, site loss 0.009596940129995346
Iteration 567, loss 1.5078771114349365, site loss 0.009749293327331543
Iteration 568, loss 1.5004289150238037, site loss 0.011624395847320557
Iteration 569, loss 1.4416639804840088, site loss 0.010262010619044304
Iteration 570, loss 1.5199227333068848, site loss 0.011402580887079239
Iteration 571, loss 1.4784512519836426, site loss 0.01014430820941925
Iteration 572, loss 1.4888936281204224, site loss 0.01188129372894764
Iteration 573, loss 1.4110827445983887, site loss 0.010558763518929482
Iteration 574, loss 1.5205793380737305, site loss 0.013989020138978958
Iteration 575, loss 1.4773941040039062, site loss 0.011941944248974323
Iteration 576, loss 1.5138251781463623, site loss 0.012570168823003769
Iteration 577, loss 1.415919303894043, site loss 0.009861404076218605
Iteration 578, loss 1.4637901782989502, site loss 0.01244920864701271
Iteration 579, loss 1.4822251796722412, site loss 0.010450182482600212
Iteration 580, loss 1.4716284275054932, site loss 0.01097910851240158
Iteration 581, loss 1.3714284896850586, site loss 0.009451238438487053
Iteration 582, loss 1.48746919631958, site loss 0.012615380808711052
Iteration 583, loss 1.6092191934585571, site loss 0.014322726987302303
Iteration 584, loss 1.5542881488800049, site loss 0.01285046897828579
Iteration 585, loss 1.5157182216644287, site loss 0.01346860732883215
Iteration 586, loss 1.5762122869491577, site loss 0.01393546536564827
Iteration 587, loss 1.5479536056518555, site loss 0.013372313231229782
Iteration 588, loss 1.522839069366455, site loss 0.012523121200501919
Iteration 589, loss 1.4564670324325562, site loss 0.010487769730389118
Iteration 590, loss 1.5172958374023438, site loss 0.013127432204782963
Iteration 591, loss 1.5973531007766724, site loss 0.010820319876074791
Iteration 592, loss 1.489320158958435, site loss 0.011216042563319206
Iteration 593, loss 1.5769271850585938, site loss 0.013842269778251648
Iteration 594, loss 1.4953703880310059, site loss 0.012971759773790836
Iteration 595, loss 1.4679160118103027, site loss 0.011740405112504959
Iteration 596, loss 1.4899272918701172, site loss 0.011406711302697659
Iteration 597, loss 1.4098057746887207, site loss 0.011425529606640339
Iteration 598, loss 1.4156298637390137, site loss 0.011465721763670444
Iteration 599, loss 1.4488434791564941, site loss 0.011123647913336754
Iteration 600, loss 1.4634695053100586, site loss 0.010980113409459591
Iteration 601, loss 1.4758117198944092, site loss 0.01191268116235733
Iteration 602, loss 1.7483233213424683, site loss 0.021083222702145576
Iteration 603, loss 1.4806675910949707, site loss 0.013427793979644775
Iteration 604, loss 1.4471142292022705, site loss 0.010647337883710861
Iteration 605, loss 1.4264687299728394, site loss 0.012001944705843925
Iteration 606, loss 1.4612667560577393, site loss 0.01138744130730629
Iteration 607, loss 1.466370701789856, site loss 0.011363329365849495
Iteration 608, loss 1.4277855157852173, site loss 0.010837153531610966
Iteration 609, loss 1.4557420015335083, site loss 0.012266063131392002
Iteration 610, loss 1.4865695238113403, site loss 0.010467705316841602
Iteration 611, loss 1.4132119417190552, site loss 0.010222900658845901
Iteration 612, loss 1.5064537525177002, site loss 0.011455144733190536
Iteration 613, loss 1.4205515384674072, site loss 0.011323986575007439
Iteration 614, loss 1.484072208404541, site loss 0.012221811339259148
Iteration 615, loss 1.4016042947769165, site loss 0.00886374618858099
Iteration 616, loss 1.4421348571777344, site loss 0.01324000395834446
Iteration 617, loss 1.459341049194336, site loss 0.013118654489517212
Iteration 618, loss 1.3835195302963257, site loss 0.00980570912361145
Iteration 619, loss 1.4179997444152832, site loss 0.010924302041530609
Iteration 620, loss 1.3662487268447876, site loss 0.010469740256667137
Iteration 621, loss 1.2860994338989258, site loss 0.008094074204564095
Iteration 622, loss 1.475611686706543, site loss 0.012845324352383614
Iteration 623, loss 1.3877549171447754, site loss 0.011340776458382607
Iteration 624, loss 1.3792476654052734, site loss 0.011421537026762962
Iteration 625, loss 1.3561487197875977, site loss 0.011312218382954597
Iteration 626, loss 1.4294743537902832, site loss 0.010781541466712952
Iteration 627, loss 1.376072883605957, site loss 0.011693311855196953
Iteration 628, loss 1.3524057865142822, site loss 0.009465884417295456
Iteration 629, loss 1.3535411357879639, site loss 0.011738497763872147
Iteration 630, loss 1.410467267036438, site loss 0.009492715820670128
Iteration 631, loss 1.4924719333648682, site loss 0.012451127171516418
Iteration 632, loss 1.476318120956421, site loss 0.011113797314465046
Iteration 633, loss 1.3498674631118774, site loss 0.009513760916888714
Iteration 634, loss 1.430752158164978, site loss 0.013917875476181507
Iteration 635, loss 1.4780075550079346, site loss 0.012825176119804382
Iteration 636, loss 1.3933324813842773, site loss 0.011691642925143242
Iteration 637, loss 1.386476755142212, site loss 0.010015007108449936
Iteration 638, loss 1.3529424667358398, site loss 0.010093929246068
Iteration 639, loss 1.2893787622451782, site loss 0.009530860930681229
Iteration 640, loss 1.3466298580169678, site loss 0.008041530847549438
Iteration 641, loss 1.4107509851455688, site loss 0.010583609342575073
Iteration 642, loss 1.2906384468078613, site loss 0.010511210188269615
Iteration 643, loss 1.4113184213638306, site loss 0.011628814041614532
Iteration 644, loss 1.3966078758239746, site loss 0.012922883033752441
Iteration 645, loss 1.4056906700134277, site loss 0.011722929775714874
Iteration 646, loss 1.4232752323150635, site loss 0.010427603498101234
Iteration 647, loss 1.3459588289260864, site loss 0.008979639038443565
Iteration 648, loss 1.430601716041565, site loss 0.011724786832928658
Iteration 649, loss 1.4194831848144531, site loss 0.012335465289652348
Iteration 650, loss 1.3012356758117676, site loss 0.008056792430579662
Iteration 651, loss 1.4154514074325562, site loss 0.009932558983564377
Iteration 652, loss 1.3708837032318115, site loss 0.008633643388748169
Iteration 653, loss 1.328811526298523, site loss 0.010059483349323273
Iteration 654, loss 1.358865737915039, site loss 0.011580219492316246
Iteration 655, loss 1.3774480819702148, site loss 0.012218987569212914
Iteration 656, loss 1.3433353900909424, site loss 0.009143698960542679
Iteration 657, loss 1.3308857679367065, site loss 0.00961911678314209
Iteration 658, loss 1.3228050470352173, site loss 0.009308629669249058
Iteration 659, loss 1.3518197536468506, site loss 0.008831793442368507
Iteration 660, loss 1.2861483097076416, site loss 0.00919165089726448
Iteration 661, loss 1.3735260963439941, site loss 0.011098196730017662
Iteration 662, loss 1.2439610958099365, site loss 0.008976692333817482
Iteration 663, loss 1.4076836109161377, site loss 0.01117565855383873
Iteration 664, loss 1.3694357872009277, site loss 0.009649250656366348
Iteration 665, loss 1.3391350507736206, site loss 0.011506917886435986
Iteration 666, loss 1.3536763191223145, site loss 0.011197330430150032
Iteration 667, loss 1.423750638961792, site loss 0.012897247448563576
Iteration 668, loss 1.447704553604126, site loss 0.010739753022789955
Iteration 669, loss 1.3928816318511963, site loss 0.010013988241553307
Iteration 670, loss 1.3282866477966309, site loss 0.01053203921765089
Iteration 671, loss 1.3440991640090942, site loss 0.009587626904249191
Iteration 672, loss 1.3729987144470215, site loss 0.010819757357239723
Iteration 673, loss 1.4089994430541992, site loss 0.009902412071824074
Iteration 674, loss 1.3976770639419556, site loss 0.01332854013890028
Iteration 675, loss 1.3282110691070557, site loss 0.007604673504829407
Iteration 676, loss 1.3932132720947266, site loss 0.009667926467955112
Iteration 677, loss 1.3924560546875, site loss 0.00977577269077301
Iteration 678, loss 1.3215970993041992, site loss 0.009435029700398445
Iteration 679, loss 1.3141387701034546, site loss 0.00975547730922699
Iteration 680, loss 1.2763307094573975, site loss 0.0076065873727202415
Iteration 681, loss 1.297175407409668, site loss 0.009296494536101818
Iteration 682, loss 1.3792660236358643, site loss 0.011944063007831573
Iteration 683, loss 1.356985330581665, site loss 0.012175315991044044
Iteration 684, loss 1.3831853866577148, site loss 0.009690322913229465
Iteration 685, loss 1.3550034761428833, site loss 0.00946944858878851
Iteration 686, loss 1.377840518951416, site loss 0.01090207789093256
Iteration 687, loss 1.332964301109314, site loss 0.01048306468874216
Iteration 688, loss 1.4100589752197266, site loss 0.01056861411780119
Iteration 689, loss 1.3834306001663208, site loss 0.008888227865099907
Iteration 690, loss 1.3696843385696411, site loss 0.011446932330727577
Iteration 691, loss 1.425376296043396, site loss 0.009938566945493221
Iteration 692, loss 1.3718593120574951, site loss 0.012476734817028046
Iteration 693, loss 1.5203994512557983, site loss 0.01051525492221117
Iteration 694, loss 1.322513222694397, site loss 0.010133355855941772
Iteration 695, loss 1.3129138946533203, site loss 0.008571602404117584
Iteration 696, loss 1.3382863998413086, site loss 0.00874386914074421
Iteration 697, loss 1.3906546831130981, site loss 0.010300112888216972
Iteration 698, loss 1.3267022371292114, site loss 0.010454872623085976
Iteration 699, loss 1.2957849502563477, site loss 0.0084056556224823
Iteration 700, loss 1.3619273900985718, site loss 0.010937342420220375
Iteration 701, loss 1.3219506740570068, site loss 0.0086694760248065
Iteration 702, loss 1.3829389810562134, site loss 0.009125368669629097
Iteration 703, loss 1.2826316356658936, site loss 0.009928053244948387
Iteration 704, loss 1.373094916343689, site loss 0.009020563215017319
Iteration 705, loss 1.3644976615905762, site loss 0.009720638394355774
Iteration 706, loss 1.301597237586975, site loss 0.008799988776445389
Iteration 707, loss 1.3280861377716064, site loss 0.009683223441243172
Iteration 708, loss 1.302895188331604, site loss 0.007990076206624508
Iteration 709, loss 1.2920308113098145, site loss 0.009903369471430779
Iteration 710, loss 1.302774429321289, site loss 0.006885955110192299
Iteration 711, loss 1.355645775794983, site loss 0.009502042084932327
Iteration 712, loss 1.389340877532959, site loss 0.008839583024382591
Iteration 713, loss 1.4542484283447266, site loss 0.011721998453140259
Iteration 714, loss 1.2736257314682007, site loss 0.0069903102703392506
Iteration 715, loss 1.4094724655151367, site loss 0.009750843048095703
Iteration 716, loss 1.3908181190490723, site loss 0.010017824359238148
Iteration 717, loss 1.37361741065979, site loss 0.009230007417500019
Iteration 718, loss 1.340656042098999, site loss 0.00821907538920641
Iteration 719, loss 1.3158270120620728, site loss 0.00874270685017109
Iteration 720, loss 1.4536372423171997, site loss 0.009736081585288048
Iteration 721, loss 1.3403575420379639, site loss 0.009800413623452187
Iteration 722, loss 1.3015272617340088, site loss 0.0070130531676113605
Iteration 723, loss 1.245365858078003, site loss 0.007149631157517433
Iteration 724, loss 1.342479944229126, site loss 0.00856709387153387
Iteration 725, loss 1.2686078548431396, site loss 0.00824009720236063
Iteration 726, loss 1.4047249555587769, site loss 0.009899070486426353
Iteration 727, loss 1.2470464706420898, site loss 0.006726922933012247
Iteration 728, loss 1.3582236766815186, site loss 0.009001722559332848
Iteration 729, loss 1.3516007661819458, site loss 0.009034791961312294
Iteration 730, loss 1.3017351627349854, site loss 0.00887927133589983
Iteration 731, loss 1.3776676654815674, site loss 0.008674397133290768
Iteration 732, loss 1.4420008659362793, site loss 0.010225672274827957
Iteration 733, loss 1.7701897621154785, site loss 0.02050555869936943
Iteration 734, loss 1.2697862386703491, site loss 0.008207698352634907
Iteration 735, loss 1.3557639122009277, site loss 0.009462707675993443
Iteration 736, loss 1.386722445487976, site loss 0.010689248330891132
Iteration 737, loss 1.3770205974578857, site loss 0.008077492006123066
Iteration 738, loss 1.365774393081665, site loss 0.007872129790484905
Iteration 739, loss 1.42559814453125, site loss 0.009957659989595413
Iteration 740, loss 1.417783260345459, site loss 0.007298130542039871
Iteration 741, loss 1.3519707918167114, site loss 0.010081796906888485
Iteration 742, loss 1.3082830905914307, site loss 0.009702111594378948
Iteration 743, loss 1.3164880275726318, site loss 0.009897889569401741
Iteration 744, loss 1.3955161571502686, site loss 0.008281512185931206
Iteration 745, loss 1.3897182941436768, site loss 0.00866615679115057
Iteration 746, loss 1.4144200086593628, site loss 0.009328034706413746
Iteration 747, loss 1.4261305332183838, site loss 0.008666737005114555
Iteration 748, loss 1.2983168363571167, site loss 0.00867820717394352
Iteration 749, loss 1.4189318418502808, site loss 0.009649574756622314
Iteration 750, loss 1.404781699180603, site loss 0.008732091635465622
Iteration 751, loss 1.3050068616867065, site loss 0.008300526067614555
Iteration 752, loss 1.3442264795303345, site loss 0.009966496378183365
Iteration 753, loss 1.342513084411621, site loss 0.009313596412539482
Iteration 754, loss 1.358224630355835, site loss 0.00845971517264843
Iteration 755, loss 1.3299860954284668, site loss 0.008636374026536942
Iteration 756, loss 1.4093517065048218, site loss 0.009371865540742874
Iteration 757, loss 1.3806921243667603, site loss 0.008253514766693115
Iteration 758, loss 1.326539397239685, site loss 0.008942343294620514
Iteration 759, loss 1.3501280546188354, site loss 0.0118296192958951
Iteration 760, loss 1.353156566619873, site loss 0.008745554834604263
Iteration 761, loss 1.419482946395874, site loss 0.00782635435461998
Iteration 762, loss 1.320544958114624, site loss 0.008235123008489609
Iteration 763, loss 1.3327288627624512, site loss 0.009234191849827766
Iteration 764, loss 1.403120756149292, site loss 0.009739539586007595
Iteration 765, loss 1.369776725769043, site loss 0.009873881936073303
Iteration 766, loss 1.3327597379684448, site loss 0.007697572931647301
Iteration 767, loss 1.2873542308807373, site loss 0.0072612641379237175
Iteration 768, loss 1.3321895599365234, site loss 0.009459157474339008
Iteration 769, loss 1.4246361255645752, site loss 0.011662580072879791
Iteration 770, loss 1.3356212377548218, site loss 0.009289713576436043
Iteration 771, loss 1.3893914222717285, site loss 0.009305472485721111
Iteration 772, loss 1.281909465789795, site loss 0.008477024734020233
Iteration 773, loss 1.3243331909179688, site loss 0.008009003475308418
Iteration 774, loss 1.465734839439392, site loss 0.011684466153383255
Iteration 775, loss 1.3326046466827393, site loss 0.008845075964927673
Iteration 776, loss 1.3729790449142456, site loss 0.009428141638636589
Iteration 777, loss 1.4493515491485596, site loss 0.011744817718863487
Iteration 778, loss 1.4431989192962646, site loss 0.009176301769912243
Iteration 779, loss 1.365199089050293, site loss 0.011201074346899986
Iteration 780, loss 1.4085288047790527, site loss 0.011267684400081635
Iteration 781, loss 1.423789620399475, site loss 0.011165954172611237
Iteration 782, loss 1.3908636569976807, site loss 0.009703490883111954
Iteration 783, loss 1.3725430965423584, site loss 0.010693265125155449
Iteration 784, loss 1.3707120418548584, site loss 0.009173614904284477
Iteration 785, loss 1.379777431488037, site loss 0.010789502412080765
Iteration 786, loss 1.343064546585083, site loss 0.01170838437974453
Iteration 787, loss 1.3070077896118164, site loss 0.010875290259718895
Iteration 788, loss 1.3585960865020752, site loss 0.010367771610617638
Iteration 789, loss 1.439410924911499, site loss 0.012451048009097576
Iteration 790, loss 1.3075652122497559, site loss 0.009611949324607849
Iteration 791, loss 1.356816291809082, site loss 0.010304881259799004
Iteration 792, loss 1.3116027116775513, site loss 0.010157229378819466
Iteration 793, loss 1.245705485343933, site loss 0.008660680614411831
Iteration 794, loss 1.2674891948699951, site loss 0.010250578634440899
Iteration 795, loss 1.3158572912216187, site loss 0.008302254602313042
Iteration 796, loss 1.3722805976867676, site loss 0.010297739878296852
Iteration 797, loss 1.2761995792388916, site loss 0.008706171996891499
Iteration 798, loss 1.3230633735656738, site loss 0.009611808694899082
Iteration 799, loss 1.356040358543396, site loss 0.00980333425104618
Iteration 800, loss 1.3532359600067139, site loss 0.010488715022802353
Iteration 801, loss 1.3240809440612793, site loss 0.008488249033689499
Iteration 802, loss 1.2724212408065796, site loss 0.008623062632977962
Iteration 803, loss 1.4510743618011475, site loss 0.00992096308618784
Iteration 804, loss 1.2896852493286133, site loss 0.008208821527659893
Iteration 805, loss 1.4806098937988281, site loss 0.016413306817412376
Iteration 806, loss 1.3715051412582397, site loss 0.011506623588502407
Iteration 807, loss 1.3196704387664795, site loss 0.010897334665060043
Iteration 808, loss 1.3613176345825195, site loss 0.011332908645272255
Iteration 809, loss 1.3039166927337646, site loss 0.009755808860063553
Iteration 810, loss 1.3280293941497803, site loss 0.010131937451660633
Iteration 811, loss 1.2954295873641968, site loss 0.009491289965808392
Iteration 812, loss 1.3134465217590332, site loss 0.008925721049308777
Iteration 813, loss 1.3629412651062012, site loss 0.009690238162875175
Iteration 814, loss 1.297654628753662, site loss 0.009651911444962025
Iteration 815, loss 1.290315866470337, site loss 0.007453517988324165
Iteration 816, loss 1.2671788930892944, site loss 0.008833120577037334
Iteration 817, loss 1.2618399858474731, site loss 0.00893053226172924
Iteration 818, loss 1.2868750095367432, site loss 0.008008872158825397
Iteration 819, loss 1.2697386741638184, site loss 0.010092569515109062
Iteration 820, loss 1.277677059173584, site loss 0.009229745715856552
Iteration 821, loss 1.307867169380188, site loss 0.007761614862829447
Iteration 822, loss 1.291129231452942, site loss 0.008044818416237831
Iteration 823, loss 1.2813150882720947, site loss 0.009345347061753273
Iteration 824, loss 1.3512989282608032, site loss 0.008599597029387951
Iteration 825, loss 1.37872314453125, site loss 0.009503183886408806
Iteration 826, loss 1.309788703918457, site loss 0.008904624730348587
Iteration 827, loss 1.2494664192199707, site loss 0.009701219387352467
Iteration 828, loss 1.3028342723846436, site loss 0.009369506500661373
Iteration 829, loss 1.3321075439453125, site loss 0.00840101670473814
Iteration 830, loss 1.329433798789978, site loss 0.009077636525034904
Iteration 831, loss 1.2738230228424072, site loss 0.009034421294927597
Iteration 832, loss 1.3091533184051514, site loss 0.01093741599470377
Iteration 833, loss 1.3376307487487793, site loss 0.009985586628317833
Iteration 834, loss 1.2748711109161377, site loss 0.007553250528872013
Iteration 835, loss 1.2094248533248901, site loss 0.006249627564102411
Iteration 836, loss 1.3739336729049683, site loss 0.010217439383268356
Iteration 837, loss 1.3216161727905273, site loss 0.010956047102808952
Iteration 838, loss 1.270459771156311, site loss 0.008410211652517319
Iteration 839, loss 1.2537240982055664, site loss 0.008786207064986229
Iteration 840, loss 1.1861307621002197, site loss 0.008928832598030567
Iteration 841, loss 1.2889823913574219, site loss 0.010080687701702118
Iteration 842, loss 1.249616265296936, site loss 0.01043158769607544
Iteration 843, loss 1.2686148881912231, site loss 0.009286965243518353
Iteration 844, loss 1.3598531484603882, site loss 0.010664300993084908
Iteration 845, loss 1.2553791999816895, site loss 0.007178374100476503
Iteration 846, loss 1.2965270280838013, site loss 0.009438890963792801
Iteration 847, loss 1.2262053489685059, site loss 0.009305858053267002
Iteration 848, loss 1.2061008214950562, site loss 0.007748143747448921
Iteration 849, loss 1.2834794521331787, site loss 0.008550313301384449
Iteration 850, loss 1.3224458694458008, site loss 0.008710719645023346
Iteration 851, loss 1.2527934312820435, site loss 0.007937831804156303
Iteration 852, loss 1.1959478855133057, site loss 0.00559790525585413
Iteration 853, loss 1.2306530475616455, site loss 0.007704542949795723
Iteration 854, loss 1.2030160427093506, site loss 0.0066367145627737045
Iteration 855, loss 1.2271959781646729, site loss 0.006486190017312765
Iteration 856, loss 1.2606439590454102, site loss 0.009023689664900303
Iteration 857, loss 1.2282917499542236, site loss 0.008823676034808159
Iteration 858, loss 1.2080867290496826, site loss 0.00873420387506485
Iteration 859, loss 1.246185064315796, site loss 0.009440016932785511
Iteration 860, loss 1.2080646753311157, site loss 0.00872752908617258
Iteration 861, loss 1.253786563873291, site loss 0.007411100901663303
Iteration 862, loss 1.25151526927948, site loss 0.009476922452449799
Iteration 863, loss 1.2355480194091797, site loss 0.008471654728055
Iteration 864, loss 1.1613006591796875, site loss 0.007721366360783577
Iteration 865, loss 1.2821781635284424, site loss 0.010562621988356113
Iteration 866, loss 1.2407530546188354, site loss 0.007178127765655518
Iteration 867, loss 1.255683183670044, site loss 0.007932040840387344
Iteration 868, loss 1.323878288269043, site loss 0.009877162985503674
Iteration 869, loss 1.2349035739898682, site loss 0.007509616669267416
Iteration 870, loss 1.337870478630066, site loss 0.0073647163808345795
Iteration 871, loss 1.2578386068344116, site loss 0.009083157405257225
Iteration 872, loss 1.26761794090271, site loss 0.010016510263085365
Iteration 873, loss 1.195300817489624, site loss 0.006725623272359371
Iteration 874, loss 1.3111011981964111, site loss 0.010576099157333374
Iteration 875, loss 1.1608326435089111, site loss 0.00778372585773468
Iteration 876, loss 1.2002918720245361, site loss 0.008661828935146332
Iteration 877, loss 1.273829698562622, site loss 0.009489381685853004
Iteration 878, loss 1.3325583934783936, site loss 0.009075295180082321
Iteration 879, loss 1.330263614654541, site loss 0.008352681994438171
Iteration 880, loss 1.2214410305023193, site loss 0.007707056123763323
Iteration 881, loss 1.2348459959030151, site loss 0.006763999816030264
Iteration 882, loss 1.2670214176177979, site loss 0.009894512593746185
Iteration 883, loss 1.2675907611846924, site loss 0.00728550786152482
Iteration 884, loss 1.2178388833999634, site loss 0.0077549973502755165
Iteration 885, loss 1.2312922477722168, site loss 0.007527866866439581
Iteration 886, loss 1.294968843460083, site loss 0.010525857098400593
Iteration 887, loss 1.2794761657714844, site loss 0.008549698628485203
Iteration 888, loss 1.2290977239608765, site loss 0.008513549342751503
Iteration 889, loss 1.2531968355178833, site loss 0.008750892244279385
Iteration 890, loss 1.2237900495529175, site loss 0.00655007641762495
Iteration 891, loss 1.2465935945510864, site loss 0.008749101310968399
Iteration 892, loss 1.2322407960891724, site loss 0.0075626675970852375
Iteration 893, loss 1.2592570781707764, site loss 0.009157637134194374
Iteration 894, loss 1.2882897853851318, site loss 0.0072530051693320274
Iteration 895, loss 1.2427829504013062, site loss 0.006874108221381903
Iteration 896, loss 1.2264692783355713, site loss 0.006969601381570101
Iteration 897, loss 1.2750272750854492, site loss 0.007891460321843624
Iteration 898, loss 1.261645793914795, site loss 0.008762507699429989
Iteration 899, loss 1.223717451095581, site loss 0.008720410987734795
Iteration 900, loss 1.1867120265960693, site loss 0.00835518166422844
Iteration 901, loss 1.2945940494537354, site loss 0.007549541536718607
Iteration 902, loss 1.1933894157409668, site loss 0.00658030342310667
Iteration 903, loss 1.2374910116195679, site loss 0.007837103679776192
Iteration 904, loss 1.1814839839935303, site loss 0.007031028624624014
Iteration 905, loss 1.1867246627807617, site loss 0.007372535299509764
Iteration 906, loss 1.2772282361984253, site loss 0.008027509786188602
Iteration 907, loss 1.2593058347702026, site loss 0.007746221497654915
Iteration 908, loss 1.1605427265167236, site loss 0.005663060583174229
Iteration 909, loss 1.1635087728500366, site loss 0.007874850183725357
Iteration 910, loss 1.2130990028381348, site loss 0.006331328302621841
Iteration 911, loss 1.2049131393432617, site loss 0.007210989017039537
Iteration 912, loss 1.228471040725708, site loss 0.009324859827756882
Iteration 913, loss 1.3062748908996582, site loss 0.008327294141054153
Iteration 914, loss 1.2021355628967285, site loss 0.006737983785569668
Iteration 915, loss 1.2055296897888184, site loss 0.007326900493353605
Iteration 916, loss 1.2047529220581055, site loss 0.00807937327772379
Iteration 917, loss 1.3362869024276733, site loss 0.010578561574220657
Iteration 918, loss 1.2158701419830322, site loss 0.006379988975822926
Iteration 919, loss 1.2820827960968018, site loss 0.007694168947637081
Iteration 920, loss 1.3320103883743286, site loss 0.010469205677509308
Iteration 921, loss 1.153056025505066, site loss 0.00690927030518651
Iteration 922, loss 1.3502309322357178, site loss 0.009493755176663399
Iteration 923, loss 1.2862716913223267, site loss 0.008994122967123985
Iteration 924, loss 1.2401736974716187, site loss 0.008825542405247688
Iteration 925, loss 1.211134910583496, site loss 0.006466269958764315
Iteration 926, loss 1.2485252618789673, site loss 0.006936565972864628
Iteration 927, loss 1.226693868637085, site loss 0.006544673815369606
Iteration 928, loss 1.3011670112609863, site loss 0.006865606643259525
Iteration 929, loss 1.1997857093811035, site loss 0.007673175539821386
Iteration 930, loss 1.339586615562439, site loss 0.0070572830736637115
Iteration 931, loss 1.2085704803466797, site loss 0.007792871445417404
Iteration 932, loss 1.2097382545471191, site loss 0.006694483570754528
Iteration 933, loss 1.2259349822998047, site loss 0.00974389910697937
Iteration 934, loss 1.4856995344161987, site loss 0.016665086150169373
Iteration 935, loss 1.2060484886169434, site loss 0.006005885079503059
Iteration 936, loss 1.379462718963623, site loss 0.009008249267935753
Iteration 937, loss 1.235626220703125, site loss 0.007710943929851055
Iteration 938, loss 1.2886563539505005, site loss 0.007395963184535503
Iteration 939, loss 1.3087018728256226, site loss 0.007251247297972441
Iteration 940, loss 1.2167210578918457, site loss 0.007695200853049755
Iteration 941, loss 1.2447004318237305, site loss 0.007383179850876331
Iteration 942, loss 1.2789990901947021, site loss 0.007065554149448872
Iteration 943, loss 1.2564244270324707, site loss 0.007870822213590145
Iteration 944, loss 1.3040108680725098, site loss 0.008471887558698654
Iteration 945, loss 1.1638169288635254, site loss 0.006282661575824022
Iteration 946, loss 1.3139770030975342, site loss 0.0076477015390992165
Iteration 947, loss 1.21038818359375, site loss 0.008351444266736507
Iteration 948, loss 1.2778277397155762, site loss 0.008535448461771011
Iteration 949, loss 1.2709193229675293, site loss 0.006542401388287544
Iteration 950, loss 1.2810896635055542, site loss 0.010150555521249771
Iteration 951, loss 1.2126455307006836, site loss 0.006166546605527401
Iteration 952, loss 1.3005881309509277, site loss 0.008166618645191193
Iteration 953, loss 1.2836658954620361, site loss 0.008963430300354958
Iteration 954, loss 1.2753570079803467, site loss 0.006850018631666899
Iteration 955, loss 1.2018452882766724, site loss 0.00678851455450058
Iteration 956, loss 1.3002232313156128, site loss 0.009134420193731785
Iteration 957, loss 1.2118337154388428, site loss 0.006519651506096125
Iteration 958, loss 1.300446629524231, site loss 0.0074068279936909676
Iteration 959, loss 1.2357914447784424, site loss 0.00751908216625452
Iteration 960, loss 1.309971809387207, site loss 0.00785258412361145
Iteration 961, loss 1.2639312744140625, site loss 0.005911266431212425
Iteration 962, loss 1.2471123933792114, site loss 0.00781412236392498
Iteration 963, loss 1.230480432510376, site loss 0.007019408047199249
Iteration 964, loss 1.3248506784439087, site loss 0.009091969579458237
Iteration 965, loss 1.1906402111053467, site loss 0.005967093165963888
Iteration 966, loss 1.238797903060913, site loss 0.009913724847137928
Iteration 967, loss 1.2779003381729126, site loss 0.008174183778464794
Iteration 968, loss 1.1783742904663086, site loss 0.007368685211986303
Iteration 969, loss 1.137895107269287, site loss 0.007900364696979523
Iteration 970, loss 1.3160759210586548, site loss 0.007252952083945274
Iteration 971, loss 1.2962806224822998, site loss 0.00742805190384388
Iteration 972, loss 1.3667521476745605, site loss 0.012031905353069305
Iteration 973, loss 1.2863855361938477, site loss 0.007688351906836033
Iteration 974, loss 1.2675540447235107, site loss 0.00912350695580244
Iteration 975, loss 1.298705816268921, site loss 0.008370221592485905
Iteration 976, loss 1.2708971500396729, site loss 0.009129328653216362
Iteration 977, loss 1.329185962677002, site loss 0.010043838061392307
Iteration 978, loss 1.2552803754806519, site loss 0.006859283894300461
Iteration 979, loss 1.2390305995941162, site loss 0.009221930056810379
Iteration 980, loss 1.2583682537078857, site loss 0.007665060926228762
Iteration 981, loss 1.2189216613769531, site loss 0.009092742577195168
Iteration 982, loss 1.1719703674316406, site loss 0.008549951016902924
Iteration 983, loss 1.3608012199401855, site loss 0.011052314192056656
Iteration 984, loss 1.2660143375396729, site loss 0.009739895351231098
Iteration 985, loss 1.291007161140442, site loss 0.008708169683814049
Iteration 986, loss 1.2947965860366821, site loss 0.008096270263195038
Iteration 987, loss 1.332273244857788, site loss 0.011023405939340591
Iteration 988, loss 1.2592179775238037, site loss 0.00850655697286129
Iteration 989, loss 1.1595818996429443, site loss 0.007654161192476749
Iteration 990, loss 1.2697944641113281, site loss 0.008667515590786934
Iteration 991, loss 1.1842764616012573, site loss 0.007390037178993225
Iteration 992, loss 1.2398016452789307, site loss 0.009765291586518288
Iteration 993, loss 1.1420457363128662, site loss 0.007320422679185867
Iteration 994, loss 1.2137559652328491, site loss 0.007711770478636026
Iteration 995, loss 1.155651569366455, site loss 0.007337414659559727
Iteration 996, loss 1.2180702686309814, site loss 0.008665360510349274
Iteration 997, loss 1.3155500888824463, site loss 0.010121224448084831
Iteration 998, loss 1.2224910259246826, site loss 0.008938631042838097
Iteration 999, loss 1.2576041221618652, site loss 0.008406534790992737
embed_matrix_0
[[ 9.4531631e-01 -6.6362572e-01 -2.4727893e-01 -7.9199553e-01
   2.6630306e-01 -6.4838982e-01  4.9781895e-01  3.4055996e-01
   2.1549630e-01 -8.5698748e-01]
 [ 3.7762475e-01  4.2707652e-01 -1.5666480e+00  1.9663838e+00
  -1.4145939e+00  2.2890637e+00 -1.1383598e+00  1.9286031e+00
   4.2996952e-01  2.1971905e+00]
 [-2.3371370e+00 -1.1667494e+00 -1.9779135e+00 -8.9416943e-02
  -3.4616404e+00  1.0992746e+00 -3.3455946e-02  5.0646788e-01
   1.6556758e+00  1.1704848e+00]
 [-3.9671674e+00 -2.5036371e-01 -2.0163848e+00  1.8442382e+00
  -1.4473797e+00  1.8149723e+00  3.8481963e-01  1.5112814e-01
  -6.4742163e-02  7.7187371e-01]
 [-2.2142050e+00 -8.4359300e-01 -2.0352168e+00 -8.1791079e-01
  -2.5947077e+00  7.7171195e-01 -6.6096812e-02  2.6684675e+00
   1.0628113e+00  1.4594271e+00]
 [-2.9565830e+00 -1.9874620e+00 -1.3260667e+00  2.5626478e+00
   8.1182522e-01  1.2457634e+00 -1.0886284e+00  2.2258182e+00
   1.6820369e+00 -1.6340098e-01]
 [-1.4169734e+00 -2.0407064e+00 -1.3329562e+00  1.9283203e+00
  -1.5640134e+00  8.9594769e-01 -1.5698057e+00  2.8287652e+00
   1.6022129e+00 -3.9462242e-01]
 [-9.2960262e-01 -1.7414360e+00 -2.2032959e+00  2.0831892e+00
  -8.8344693e-01  5.1150095e-01  8.2282043e-01  1.5926155e+00
  -2.7255625e-01  2.5623534e+00]
 [-5.9935945e-01 -1.6447103e+00 -2.8671434e+00  2.2154741e+00
  -4.7831061e-01  9.6427572e-01 -2.5423193e-01  8.0679041e-01
  -2.0589176e-01  2.5838077e+00]
 [-2.9381766e+00  3.7434462e-01 -2.6715438e+00  1.7254766e+00
   3.1119075e-01  7.1070772e-01  3.5934386e-01  1.7745707e+00
   1.3858186e+00  1.9498841e+00]
 [-7.0174545e-01 -5.4487544e-01 -2.6685939e+00  1.8239434e+00
  -2.2617886e+00  8.9570570e-01 -3.6711860e-01  1.8699018e+00
   1.6497146e+00  7.2135198e-01]
 [-4.1019654e+00 -2.0570648e+00  3.3074844e-01  1.8603436e+00
  -1.4391742e+00  1.4775004e+00 -1.3660127e+00 -2.5722854e+00
   9.8942153e-02  3.1414404e+00]
 [ 1.4961120e-01 -2.0231092e-01  4.6213919e-01 -3.6523953e-01
  -1.1599414e+00  2.2692247e-03 -1.1504644e+00 -1.3178416e-01
  -6.8522626e-01  6.5593243e-01]
 [-1.8418289e+00 -6.6648161e-01  1.5376490e+00  2.5998120e+00
  -1.3605092e+00  9.7436881e-01  1.9595506e+00  2.2429178e+00
   1.8423136e+00  2.3901501e+00]
 [ 1.5016469e-01 -3.3753610e+00 -1.4061821e+00 -5.8636516e-01
   5.2290404e-01  2.0931160e+00  5.2520506e-02  5.5606824e-01
   3.8791475e+00  3.3191648e+00]
 [-7.2067940e-01 -2.3224456e+00 -2.3113365e-01  2.1048641e+00
  -2.2388754e+00  3.6143559e-01 -7.9672384e-01  1.3770143e+00
  -1.7950892e-01  2.4175024e+00]
 [-1.2332218e+00 -9.5947284e-01 -1.3552174e-01  6.1031729e-01
  -8.6814147e-01  2.2168720e+00 -2.1821663e+00  1.8113648e+00
  -3.0988351e-01  2.7280180e+00]
 [-1.0766020e+00  1.2273632e-01 -4.7814202e-01  4.8460668e-01
  -6.7598099e-01  2.7359445e+00 -1.9684308e+00  2.0016556e+00
  -8.2934992e-03  2.8179731e+00]
 [-6.1339641e-01 -5.3130507e-01 -1.0319999e+00  1.4918594e+00
  -1.3197865e+00  4.4455147e+00  5.5050850e-01  9.8250443e-01
  -8.0672169e-01  1.3525966e+00]
 [-2.0263438e+00 -1.3266017e+00 -1.1322531e+00  7.8437334e-01
  -1.1429154e+00  4.4285364e+00  9.5286059e-01  1.3051000e+00
  -1.3950238e+00  5.2875614e-01]
 [-4.7860402e-01 -2.2990046e+00 -1.3593262e-01  2.0681994e+00
  -2.7529600e+00  5.1427245e+00  4.0405893e-01 -5.8547372e-01
   8.1472117e-01 -6.1989737e-01]
 [-3.4724772e-01  8.0818057e-01 -9.9481380e-01  1.9172970e+00
  -1.5859660e+00  2.7214689e+00 -1.7099200e+00  6.2572706e-01
   1.1296102e+00  2.3917117e+00]
 [ 4.8707011e-01 -4.6883270e-01  8.4259784e-01 -3.7028305e-03
   8.3193463e-01  3.6631551e-01  4.4701961e-01  4.6858677e-01
  -6.7318869e-01  4.6946589e-02]
 [-2.4841523e-01 -7.8847647e-02 -9.7334170e-01  2.6094627e-01
   2.7083182e-01 -6.1169386e-02 -3.5592604e-01 -9.2001724e-01
   4.5677471e-01 -3.1811309e-01]
 [ 2.7690740e+00  1.8335147e+00 -2.2809772e+00 -7.9212528e-01
  -1.8822223e+00  4.1183896e+00  2.2705779e+00 -1.2475266e+00
   3.5286033e+00 -1.1908730e+00]
 [-4.0053439e-01 -4.2918682e-01 -6.9176984e-01  6.9427896e-01
   7.9518843e-01 -4.0332770e-01 -5.2031207e-01  6.7869163e-01
  -2.5721478e-01  5.9246588e-01]]
rnn/mlstm/mlstm/wx_0
[[ 0.10048319  0.20249149 -0.45981032 ...  0.50959647  0.36885247
  -0.05032751]
 [-0.03255828  0.23569642  0.0533215  ...  0.4096343   0.10307093
  -0.04701159]
 [ 0.5246924   0.29806393 -0.12241385 ...  0.47790697  0.13609907
   0.09513337]
 ...
 [ 0.29815382 -0.4876087   0.16204625 ... -0.37171507 -0.49606422
  -0.1622233 ]
 [ 0.2870698  -0.6022514   0.13478088 ... -0.28780988  0.09961881
   0.06071072]
 [-0.21338117 -0.46589154 -0.0548175  ... -0.07141599 -0.13871096
  -0.30051807]]
rnn/mlstm/mlstm/wh_0
[[-0.10265569 -0.18454725 -0.07758266 ...  0.0430256  -0.05058567
  -0.0581021 ]
 [-0.0047196  -0.08749159 -0.06819361 ... -0.10511478 -0.08746262
   0.05891916]
 [-0.04603881 -0.12037263 -0.0068205  ...  0.09866963  0.06504888
   0.11612732]
 ...
 [ 0.08260071  0.16531943 -0.04229071 ... -0.10501474  0.00618196
   0.02929503]
 [-0.0091562  -0.00960845 -0.02448618 ...  0.0003112  -0.02279539
   0.01199591]
 [-0.0107499   0.04505077 -0.01730925 ... -0.15140048  0.00505834
   0.0260949 ]]
rnn/mlstm/mlstm/wmx_0
[[ 0.38450697  0.7491759   0.09219804 ...  1.0303712   0.14872287
  -0.10363264]
 [ 0.25580174  0.12117297 -1.1010351  ...  0.04512994  0.08058687
  -0.05333899]
 [-0.02860186 -0.31605735  0.09719585 ... -0.08137119  0.24538785
   0.1878594 ]
 ...
 [ 0.15421247  0.6208773  -0.06776959 ...  0.09007471 -0.1870258
   0.48085266]
 [ 0.33057833 -0.0916146  -0.25746593 ... -0.09292378  0.00348752
  -0.03997805]
 [-0.4707368  -0.25028792  0.04240104 ...  0.00112198 -0.31279603
   0.44959995]]
rnn/mlstm/mlstm/wmh_0
[[ 0.14701945 -0.15841374 -0.05914005 ... -0.08215918 -0.01665653
  -0.02861892]
 [ 0.08898497 -0.03984508 -0.05059856 ... -0.0838673   0.08649793
  -0.0711226 ]
 [ 0.02169273  0.00094164 -0.06107165 ... -0.00517822 -0.09926154
  -0.04676931]
 ...
 [ 0.00684343 -0.02316095 -0.00059623 ...  0.10614545  0.01306034
  -0.0551053 ]
 [-0.06224521 -0.06602556 -0.00353187 ...  0.1252848  -0.06443079
   0.06901939]
 [-0.01810624  0.0777278  -0.03517171 ... -0.1941373   0.04122957
   0.14816011]]
rnn/mlstm/mlstm/b_0
[2.8846056 2.7643569 2.9300942 ... 3.0360014 3.0144367 2.9884706]
rnn/mlstm/mlstm/gx_0
[0.61945504 0.8333323  0.99397206 ... 0.8379356  0.78993034 0.9055399 ]
rnn/mlstm/mlstm/gh_0
[1.6192625  1.1113647  1.3372495  ... 0.95362514 1.0064565  0.9807314 ]
rnn/mlstm/mlstm/gmx_0
[1.5899814 1.8164955 1.9343221 ... 1.1993469 1.2900522 1.3764164]
rnn/mlstm/mlstm/gmh_0
[1.5896268 1.8154762 1.9334326 ... 1.1988682 1.289796  1.3759233]
fully_connected/weights_0
[[-0.10474175 -0.22555856 -0.30415457 ... -0.11482231 -0.16449519
  -0.5560854 ]
 [ 0.03222087 -0.01189191 -0.03752938 ...  0.01386477  0.00382517
   0.06453051]
 [-0.12529288 -0.28049013 -0.32713667 ... -0.01147071  0.05370649
  -0.01859566]
 ...
 [-0.04035051  0.00887146 -0.00493755 ...  0.00464287  0.00646375
  -0.09237419]
 [ 0.02820693  0.0187585  -0.05965707 ...  0.00506994  0.02136058
  -0.03070873]
 [-0.20257027  0.0678217  -0.07843481 ...  0.09034383  0.07699694
   0.02864904]]
fully_connected/biases_0
[ -1.6626253   -0.19238056  -1.2346075   -0.04837455  -0.6038908
  -0.31929788  -0.24760352  -0.8645084   -0.8043198   -0.96342874
  -1.3250101   -9.861444    -0.5702089   -0.4182143   -0.8935436
  -0.98940295  -1.1603819   -1.3404244   -1.4627982   -1.9649829
  -0.63666314 -10.215276   -10.215427   -10.208282     1.2841522 ]
fully_connected_1/weights_0
[[-0.04771633  0.04979074  0.01041197]
 [-0.0084759  -0.01793955  0.03418843]
 [ 0.08131944  0.04040453 -0.11156628]
 ...
 [ 0.01108642 -0.04555387  0.04516435]
 [ 0.0735616  -0.02792254 -0.03400868]
 [-0.04927791 -0.00393944  0.03279999]]
fully_connected_1/biases_0
[ 0.01517697 -0.01853584 -0.01197146]
Traceback (most recent call last):
  File "train_script.py", line 152, in <module>
    np.save('{args.save_dir}/losses.npy', losses)
  File "/global/software/sl-7.x86_64/modules/apps/ml/tensorflow/1.12.0-py36/lib/python3.6/site-packages/numpy/lib/npyio.py", line 524, in save
    fid = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: '{args.save_dir}/losses.npy'
